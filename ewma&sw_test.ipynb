#%% raw
import time
import numpy as np
import math
import matplotlib.pyplot as plt
import pickle
%matplotlib inline
#%% raw
%load_ext autoreload
%autoreload 2
import kl_ucb_policy
#%% raw
#Parameters for EWMA&SW test
p = np.array([.99, .98, .96, .93, .90, .10, .06, .04]) #Bernoulli Parameters
# p = np.array([.95, .90, .80, .65, .45, .25, .15, .10])
# p = np.array([.90, .80, .70, .55, .45, .35, .20, .10])
rate = np.array([6, 9, 12, 18, 24, 36, 48, 54])
tp = np.multiply(p,rate)

K= p.shape[0] #Number of branches
delta= ( np.ones(K)*np.max(tp) ) - tp

T= 5000 #Time periods
runs = 10 #Number of iterations

print("Pi:",p)
print("delta:",delta)
#%% raw
## KL-UCB EWMA & SW
#%% raw
ewma = kl_ucb_policy.KLUCB_EWMA(K, rate, 0.1) #Optimal Graphical OptimalRate Sampling
total_rewards_list_ewma = np.zeros((runs, T))
actions_list_ewma = []
sw = kl_ucb_policy.KLUCB_SW(K, rate, 500) #Original KL UCB
total_rewards_list_sw = np.zeros((runs, T))
actions_list_sw = []
start_time = time.time()

for run in range(runs):
    ewma.reset()
    actions_ewma = np.zeros((K, T), dtype=np.int)
    rewards_ewma = np.zeros((K, T), dtype=np.float)
    sw.reset()
    actions_sw = np.zeros((K, T), dtype=np.int)
    rewards_sw = np.zeros((K, T), dtype=np.float)
    for t in range(T):
        arm_ewma = ewma.select_next_arm()
        actions_ewma[arm_ewma, t] = 1
        rewards_ewma[arm_ewma, t] = np.random.binomial(1, p[arm_ewma]) * rate[arm_ewma]
        ewma.update_state(arm_ewma, rewards_ewma[arm_ewma, t] / rate[arm_ewma])

        arm_sw = sw.select_next_arm()
        actions_sw[arm_sw, t] = 1
        rewards_sw[arm_sw, t] = np.random.binomial(1, p[arm_sw]) * rate[arm_sw]
        sw.update_state(arm_sw, rewards_sw[arm_sw, t] / rate[arm_sw])

    cumulative_rewards_ewma = np.cumsum(rewards_ewma, axis=1) #Cumulative rewards of each arm according to time
    total_rewards_ewma = np.sum(cumulative_rewards_ewma, axis=0) #Cumulative rewards of all arms according to time
    total_rewards_list_ewma[run, :] = np.copy(total_rewards_ewma)
    actions_list_ewma.append(np.copy(actions_ewma))

    cumulative_rewards_sw = np.cumsum(rewards_sw, axis=1)
    total_rewards_sw = np.sum(cumulative_rewards_sw, axis=0)
    total_rewards_list_sw[run, :] = np.copy(total_rewards_sw)
    actions_list_sw.append(np.copy(actions_sw))

time_spent = time.time() - start_time
#%% raw
## Results
#%% raw
print("Time for KL-UCB (Newton), with T =", T, ", runs =", runs, ":", (time_spent), "seconds")
#%% raw
## Average total rewards
#%% raw
mean_total_rewards_ewma = np.mean(total_rewards_list_ewma, axis=0)
mean_total_rewards_sw = np.mean(total_rewards_list_sw, axis=0)
print("t & Reward (total) G-ORS :", t, mean_total_rewards_ewma[t])
print("t & Reward (total) KL-UCB :", t, mean_total_rewards_sw[t])
#%% raw
fig=plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(1,1,1)
ax1.plot(mean_total_rewards_ewma, linestyle='-', label='Average total rewards of EWMA')
ax1.plot(mean_total_rewards_sw, linestyle='-', label='Average total rewards of SW')
ax1.legend(loc='best')
plt.show()
#%% raw
#Regrets calculation for G-ORS
total_action_ewma = np.zeros((K, T))
for actions_ewma in actions_list_ewma:
    total_action_ewma += np.cumsum(actions_ewma, axis=1) #The cumulative times of each arm to be selected
total_action_ewma = total_action_ewma / runs
regret_cumule_ewma = np.dot(delta, total_action_ewma[:, :]) # Cumulative regrets
#Regrest calculation for KL-UCB
total_action_sw = np.zeros((K, T))
for actions_sw in actions_list_sw:
    total_action_sw += np.cumsum(actions_sw, axis=1) #The cumulative times of each arm to be selected
total_action_sw = total_action_sw / runs
regret_cumule_klucb = np.dot(delta, total_action_sw[:, :]) # Cumulative regrets
#%% raw
## Average cumulative regrets
#%% raw
fig=plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(1,1,1)

ax1.plot(regret_cumule_ewma[:], linestyle='-', label='EWMA')
ax1.plot(regret_cumule_klucb[:],linestyle='-', label='SW')

ax1.legend(loc='best')
ax1.grid()
ax1.set_title('Evolution of average cumulative regrets according to time')
plt.show()
#%% raw
T0 = 4999
#G-ORS
distribution_regret_gors = []
distribution_regret_klucb = []
for actions_gors in actions_list_ewma:
    distribution_regret_gors.append(np.dot(delta, np.sum(actions_gors[:, :T0], axis=1)))
for actions_sw in actions_list_sw:
    distribution_regret_klucb.append(np.dot(delta, np.sum(actions_sw[:, :T0], axis=1)))
#%% raw
## Boxplot of average regret
#%% raw
fig=plt.figure(figsize=(12,8))
ax = fig.add_subplot(1,1,1)

ax.boxplot([
        distribution_regret_gors,
        distribution_regret_klucb
    ])
ax.set_xticklabels([
        'EWMA',
        'SW'
    ])

ax.set_title('Boxplot of regret when t = 5000')
plt.show()
